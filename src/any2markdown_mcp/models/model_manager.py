"""
æ¨¡å‹ç®¡ç†å™¨æ¨¡å—

è´Ÿè´£åŠ è½½å’Œç®¡ç†marker-pdfæ¨¡å‹ä»¥åŠå…¶ä»–å¿…è¦çš„æœºå™¨å­¦ä¹ æ¨¡å‹
æ”¯æŒGPU/CPUè‡ªåŠ¨æ£€æµ‹å’Œæ¨¡å‹ç¼“å­˜
"""

import asyncio
import os
import sys
from pathlib import Path
from typing import Optional, Dict, Any
import warnings
import time

import structlog
import torch
from huggingface_hub import snapshot_download

# æŠ‘åˆ¶ä¸€äº›ä¸å¿…è¦çš„è­¦å‘Š
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)

logger = structlog.get_logger(__name__)


class ModelManager:
    """æ¨¡å‹ç®¡ç†å™¨ï¼Œè´Ÿè´£åŠ è½½å’Œç®¡ç†æ‰€æœ‰æœºå™¨å­¦ä¹ æ¨¡å‹"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.device = self._detect_device()
        self.models: Dict[str, Any] = {}
        self.model_cache_dir = Path(config.get("model_cache_dir", "~/.cache/marker")).expanduser()
        self.model_cache_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–æ ‡å¿—
        self._initialized = False
        self._initialization_lock = asyncio.Lock()
        
        # è®¾ç½®æ¨¡å‹ä¸‹è½½ç›¸å…³çš„ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿è¿›åº¦æ˜¾ç¤ºï¼‰
        self._setup_model_env_vars()
        
        logger.info("ModelManager initialized", 
                   device=self.device, 
                   cache_dir=str(self.model_cache_dir),
                   progress_bars_enabled=not self._is_progress_disabled())
    
    def _setup_model_env_vars(self):
        """è®¾ç½®æ¨¡å‹ä¸‹è½½ç›¸å…³çš„ç¯å¢ƒå˜é‡"""
        # ç¡®ä¿æ˜¾ç¤ºä¸‹è½½è¿›åº¦æ¡
        if self.config.get("hf_hub_disable_progress_bars", False):
            logger.warning("âš ï¸  Progress bars are disabled in config. Model download progress will not be visible!")
            os.environ["HF_HUB_DISABLE_PROGRESS_BARS"] = "true"
        else:
            logger.info("âœ… Model download progress bars are enabled")
            os.environ["HF_HUB_DISABLE_PROGRESS_BARS"] = "false"
        
        # è®¾ç½®å…¶ä»–ç¯å¢ƒå˜é‡
        env_vars = {
            "MODEL_CACHE_DIR": self.config.get("model_cache_dir", "~/.cache/marker"),
            "HF_HOME": self.config.get("hf_home", "~/.cache/huggingface"),
            "HF_HUB_CACHE": self.config.get("hf_hub_cache", "~/.cache/huggingface/hub"),
            "HF_ASSETS_CACHE": self.config.get("hf_assets_cache", "~/.cache/huggingface/assets"),
            "TORCH_HOME": self.config.get("torch_home", "~/.cache/torch"),
            "TRANSFORMERS_CACHE": self.config.get("transformers_cache", "~/.cache/transformers"),
            "HF_HUB_ENABLE_HF_TRANSFER": str(self.config.get("hf_hub_enable_hf_transfer", False)).lower(),
            "HF_HUB_DISABLE_TELEMETRY": str(self.config.get("hf_hub_disable_telemetry", True)).lower(),
        }
        
        for key, value in env_vars.items():
            # å±•å¼€ç”¨æˆ·ç›®å½•è·¯å¾„
            if value.startswith("~"):
                expanded_value = str(Path(value).expanduser())
            else:
                expanded_value = value
            
            # è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆå³ä½¿å·²å­˜åœ¨ä¹Ÿè¦æ›´æ–°ï¼Œç¡®ä¿ä½¿ç”¨é…ç½®ä¸­çš„å€¼ï¼‰
            os.environ[key] = expanded_value
            logger.debug("Set environment variable", var=key, value=expanded_value)
        
        # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
        for key in ["HF_HOME", "HF_HUB_CACHE", "HF_ASSETS_CACHE", "TORCH_HOME", "TRANSFORMERS_CACHE", "MODEL_CACHE_DIR"]:
            cache_dir = Path(os.environ[key])
            cache_dir.mkdir(parents=True, exist_ok=True)
            logger.debug("Ensured cache directory exists", dir=str(cache_dir))
    
    def _is_progress_disabled(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦ç¦ç”¨äº†è¿›åº¦æ¡"""
        return os.environ.get("HF_HUB_DISABLE_PROGRESS_BARS", "false").lower() == "true"
    
    def _detect_device(self) -> str:
        """è‡ªåŠ¨æ£€æµ‹æœ€ä½³è®¾å¤‡"""
        device_config = self.config.get("device", "auto").lower()
        
        if device_config == "cpu":
            return "cpu"
        elif device_config == "cuda" and torch.cuda.is_available():
            return "cuda"
        elif device_config == "mps" and torch.backends.mps.is_available():
            return "mps"
        elif device_config == "auto":
            # è‡ªåŠ¨æ£€æµ‹æœ€ä½³è®¾å¤‡
            if torch.cuda.is_available():
                device = "cuda"
                gpu_count = torch.cuda.device_count()
                gpu_name = torch.cuda.get_device_name(0)
                logger.info("CUDA available", gpu_count=gpu_count, gpu_name=gpu_name)
                return device
            elif torch.backends.mps.is_available():
                logger.info("MPS (Apple Silicon) available")
                return "mps"
            else:
                logger.info("Using CPU (no GPU acceleration available)")
                return "cpu"
        else:
            logger.warning("Invalid device config, falling back to CPU", 
                         device_config=device_config)
            return "cpu"
    
    async def initialize(self) -> None:
        """å¼‚æ­¥åˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹"""
        if self._initialized:
            return
            
        async with self._initialization_lock:
            if self._initialized:  # åŒé‡æ£€æŸ¥
                return
                
            logger.info("ğŸš€ Starting model initialization...")
            logger.info("ğŸ“¥ This may take some time for first run as models need to be downloaded...")
            
            start_time = time.time()
            
            try:
                # åˆå§‹åŒ–markeræ¨¡å‹
                await self._initialize_marker_model()
                
                # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ å…¶ä»–æ¨¡å‹çš„åˆå§‹åŒ–
                # await self._initialize_other_models()
                
                self._initialized = True
                
                elapsed_time = time.time() - start_time
                logger.info("âœ… All models initialized successfully", 
                          initialization_time=f"{elapsed_time:.2f}s")
                
            except Exception as e:
                elapsed_time = time.time() - start_time
                logger.error("âŒ Failed to initialize models", 
                           error=str(e), 
                           elapsed_time=f"{elapsed_time:.2f}s")
                raise
    
    async def _initialize_marker_model(self) -> None:
        """åˆå§‹åŒ–marker-pdfæ¨¡å‹"""
        try:
            logger.info("ğŸ“¦ Loading marker-pdf models...")
            logger.info("â„¹ï¸  If this is the first run, marker will download ~3-5GB of AI models")
            logger.info("ğŸ“Š Download progress will be shown below:")
            
            # å¯¼å…¥markeræ¨¡å—ï¼ˆå»¶è¿Ÿå¯¼å…¥ä»¥é¿å…å¯åŠ¨æ—¶çš„ä¾èµ–é—®é¢˜ï¼‰
            try:
                from marker.scripts.convert import process_single_pdf, create_model_dict
                logger.info("âœ… Marker modules imported successfully")
            except ImportError as e:
                logger.error("âŒ Failed to import marker modules", error=str(e))
                logger.error("ğŸ’¡ Please ensure marker-pdf is installed: pip install marker-pdf")
                raise
            
            # åœ¨executorä¸­è¿è¡Œæ¨¡å‹åŠ è½½ï¼ˆé¿å…é˜»å¡äº‹ä»¶å¾ªç¯ï¼‰
            loop = asyncio.get_event_loop()
            
            logger.info("ğŸ”„ Loading AI models (this may take several minutes)...")
            models = await loop.run_in_executor(
                None, 
                self._load_marker_models
            )
            
            self.models["marker"] = {
                "models": models,
                "convert_func": process_single_pdf
            }
            
            logger.info("âœ… Marker models loaded successfully")
            
        except ImportError as e:
            logger.error("âŒ Failed to import marker modules", error=str(e))
            raise
        except Exception as e:
            logger.error("âŒ Failed to load marker models", error=str(e))
            raise
    
    def _load_marker_models(self) -> Any:
        """åœ¨çº¿ç¨‹æ± ä¸­åŠ è½½markeræ¨¡å‹"""
        try:
            from marker.scripts.convert import create_model_dict
            
            logger.info("ğŸ”§ Setting up model cache directories...")
            
            # è®¾ç½®ç¯å¢ƒå˜é‡
            os.environ["TORCH_HOME"] = str(self.model_cache_dir)
            
            logger.info("ğŸ¤– Creating marker model dictionary...")
            logger.info("ğŸ“¡ Models will be downloaded to:", cache_dir=str(self.model_cache_dir))
            
            # åˆ›å»ºæ¨¡å‹å­—å…¸
            device = None if self.device == "auto" else self.device
            logger.info("ğŸš€ Initializing models...", target_device=device or "auto")
            
            models = create_model_dict(device=device)
            
            logger.info("âœ… Marker models created successfully")
            return models
            
        except Exception as e:
            logger.error("âŒ Error in _load_marker_models", error=str(e))
            import traceback
            logger.error("ğŸ” Full traceback:", traceback=traceback.format_exc())
            raise
    
    async def get_marker_models(self) -> Dict[str, Any]:
        """è·å–markeræ¨¡å‹"""
        await self.initialize()
        return self.models["marker"]["models"]
    
    async def convert_pdf_with_marker(self, pdf_path: str, **kwargs) -> tuple:
        """ä½¿ç”¨markerè½¬æ¢PDF"""
        await self.initialize()
        
        try:
            convert_func = self.models["marker"]["convert_func"]
            models = self.models["marker"]["models"]
            
            # åœ¨executorä¸­è¿è¡Œè½¬æ¢ï¼ˆé¿å…é˜»å¡äº‹ä»¶å¾ªç¯ï¼‰
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                None,
                lambda: convert_func(pdf_path, models, **kwargs)
            )
            
            return result
            
        except Exception as e:
            logger.error("PDF conversion failed", error=str(e), pdf_path=pdf_path)
            raise
    
    def get_device_info(self) -> Dict[str, Any]:
        """è·å–è®¾å¤‡ä¿¡æ¯"""
        info = {
            "device": self.device,
            "torch_version": torch.__version__,
        }
        
        if self.device == "cuda":
            info.update({
                "cuda_available": torch.cuda.is_available(),
                "cuda_version": torch.version.cuda,
                "gpu_count": torch.cuda.device_count(),
                "gpu_names": [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())],
                "current_device": torch.cuda.current_device(),
            })
        elif self.device == "mps":
            info.update({
                "mps_available": torch.backends.mps.is_available(),
            })
        
        return info
    
    async def cleanup(self) -> None:
        """æ¸…ç†èµ„æº"""
        logger.info("Cleaning up model manager...")
        
        # æ¸…ç†GPUç¼“å­˜
        if self.device == "cuda" and torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # æ¸…ç†æ¨¡å‹å¼•ç”¨
        self.models.clear()
        self._initialized = False
        
        logger.info("Model manager cleanup completed")
    
    def __del__(self):
        """ææ„å‡½æ•°"""
        if hasattr(self, 'models') and self.models:
            # æ³¨æ„ï¼šåœ¨ææ„å‡½æ•°ä¸­ä¸èƒ½ä½¿ç”¨async
            if self.device == "cuda" and torch.cuda.is_available():
                torch.cuda.empty_cache() 